# Краткое описание статей:

### ImageNet classification with deep convolutional neural networks

Одна из классических статей, с которых начался современный виток диплернинга.

Появление ImageNet позволило решать задачу классификации изображений на
качественно новом уровне.

Авторы использовали сверточную сеть из 8 слоев: 5 сверточных, 3 полносвязных,
последний слой - softmax на 1000 классов.

Обучали на двух GPU GTX 580 (по 3Gb памяти) в течении 6 дней.

Функция активации: ReLU

Функция потерь: множественная логистическая регрессия.

Так же использовали локальную нормализацию (local normalization).

Для борьбы с оверфитом применили аугментации и дропаут.

Полученные результаты:
- ошибка на top-1: 35.7%
- ошибка на top-5: 17.0%

<hr />

### Very deep convolutional networks for large-scale image recognition

Вторая классическая статья по сверточным сетям. Ей мы обязаны появлением VGG-16 и VGG-19.

Сеть организована по тем же принципам что в вышеуказанной статье.
Ключевой особенностью является использование сверток 3x3, что позволило увеличить глубину до 16-19 слоев.

Два слоя сверток 3x3 заменяют один 5x5. Три слоя 3x3 заменяют 7x7.
Использование нескольких маленьких сверток вместо одной большой позволяет
увеличить число нелинейных преобразований и значительно уменьшить количество
параметров сети.

После сверточных слоев - два полносвязных слоя на 4096 нейронов и последний
софтмакс слой на 1000.
В качестве функции активации ReLU.
От использования локальной нормализации отказались.

Сеть обучали на оптимизацию множественной логистической регрессии,
используя градиентный спуск с импульсом и L2 регуляризацию.
Реализацию собрали на Caffee, учили 2-3 недели на 4х NVIDIA Titan Black.

Лучшие результаты, полученные с помощью 1 сети:
- ошибка на топ-1: 24.4%
- ошибка на топ-5: 7.1%

Потом из нескольких сетей сделали ансамбли.
Усреднение softmax классов нескольких сетей дало:
- ошибка на топ-1: 23.7%
- ошибка на топ-5: 6.8%

Для применения сети к задаче локализации логистическую регрессию
заменили на Евклидовым расстоянием. Получили результаты превосходящие state of the art
на момент написания статьи.

Для работы с другими датасетами выкинули последний полносвязный слой и использовали 4096 выходов
предпоследнего слоя в качестве признаков изображения. Сверху добавили SVM с L2.
Веса остальных слоев не меняли. На новых датасетах был получен результат аналогичный либо превосходящий state of the art.

<hr />

### Going Deeper with Convolutions

Статья про GoogleLeNet 2014 года.
Архитектура сети получила название `Inception` (we need to go deeper).

Главное отличие `Inception` архитектуры в использовании новых `Inception` слоев.

Самый очевидный путь улучшения производительности нейронной сети - увеличение ее размера.
Но больший размер сети, обычно, влечет увеличение числа параметров.
Значит выше шанс оверфитнуться и ресурсов нужно больше.

Эти проблемы можно решить используя разряженные слои вместо полносвязных.
Но современное железо плохо справляется с разряженными структурами данных (кеш промахи).

Ключевая идея `Inception` - выяснить, как оптимальные локально-разряженные структуры
сверточной сети могут быть аппроксимированы существующими неразряженными компонентами.

`Inception` слой одновременно применяет к предыдущему слою
 1x1, 3x3 и 5x5 свертки, 3x3 пулинг и комбинирует результаты.
 Это проще один раз [увидеть](https://www.youtube.com/watch?v=VxhSouuSZDY).
Для уменьшения вычислительной сложности дополнительно используются 1x1 свертки.


GoogleLeNet является конкретной реализацией `Inception` архитектуры, использованной
в соревновании ILSVRC 2014.

Сеть содержит 22 слоя (27 если считать пулинг слои).
На самом деле простых независимых слоев около 100.

Функция активации: ReLU

Обучали стохастическим градиентным спуском с импульсом.

Для улучшения backpropogation дополнительно использовали вспомогательные классификаторы
на основе простых сверточных сетей.

Тренировка: Обучили 7 одинаковых сетей, с разным семплингом и рандомизацией входных изображений. Объединили в ансамбль, получили state of the art.

Ошибка на топ-5: 6.67%

Затем GoogleLeNet применили на детекции объектов, ансамбль из 6 сетей показал state of the art.

<hr />

### Using Deep Learning and Google Street View to Estimate the Demographic Makeup of the US

Исследовали использовали снимки с Google Street View, локализовали автомобили,
с помощью сверточных сетей определили марку, модель и год выпуска авто и на основе этого получали демографические данные по району: национальность, образование, доход, за кого голосуют.

*For instance, if the number of sedans encountered during a 15-minute drive through a city is higher than the number
of pickup trucks, the city is likely to vote for a Democrat during the next Presidential
election (88% chance); otherwise, it is likely to vote Republican (82%).*

<hr />
